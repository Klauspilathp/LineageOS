# kafka 配置
spring.kafka.bootstrap-servers=127.0.0.1:8000
spring.kafka.listener.type=BATCH
# producer
spring.kafka.producer.batch-size=500
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=
# consumer
spring.kafka.consumer.group-id=defaultGroup
# earliest：当各分区下有已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，从头开始消费。
# latest：当各分区下有已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，消费新产生的该分区下的数据。
# none：topic 各分区都存在已提交的 offset 时，从 offset 后开始消费；只要有一个分区不存在已提交的 offset 则抛出异常。
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.consumer.value-deserializer=

# kafka streams
####### produce
# spring.cloud.stream.kafka.streams.binder.brokers=localhost
# spring.cloud.stream.kafka.streams.binder.configuration.default.key.serde=org.apache.kafka.common.serialization.Serdes$StringSerde
# spring.cloud.stream.kafka.streams.binder.configuration.default.value.serde=org.apache.kafka.common.serialization.Serdes$BytesSerde
# spring.cloud.stream.kafka.streams.binder.configuration.commit.interval.ms=1000
# spring.cloud.stream.bindings.outputOrder.binder=kafka
# spring.cloud.stream.bindings.outputOrder.destination=order
####### consume
# spring.cloud.stream.kafka.streams.binder.brokers=localhost
# spring.cloud.stream.kafka.streams.binder.configuration.default.key.serde=org.apache.kafka.common.serialization.Serdes$StringSerde
# spring.cloud.stream.kafka.streams.binder.configuration.default.value.serde=org.apache.kafka.common.serialization.Serdes$BytesSerde
# spring.cloud.stream.kafka.streams.binder.configuration.commit.interval.ms=1000
# spring.cloud.stream.bindings.inputOrder.binder=kafka
# spring.cloud.stream.bindings.inputOrder.destination=order
